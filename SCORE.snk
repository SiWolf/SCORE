# ---------------------------------------------------------------------------
# Title: SCORE
# Author: Silver A. Wolf
# Last Modified: Fr, 09.08.2019
# Version: 0.6.5
# Usage:
#		time snakemake -s "SCORE.snk" -j {max_amount_of_threads} --use-conda
# Additional options:
#		sequanix
#       snakemake -s "SCORE.snk" -n
#       snakemake -s "SCORE.snk" --config {parameter}={value}
#       snakemake -s "SCORE.snk" --forceall --dag | dot -Tsvg > dag.svg
# ---------------------------------------------------------------------------

# Imports
import csv

# Specifying the config file
configfile: "config.yaml"

# Global parameters
ENABLE_BENCHMARKING = config["benchmarking"]
EXPERIMENT_NAME = config["analysis_name"]
METADATA = config["metadata_file"]
MERGE_PLOTS_AND_PDF = config["merge_plots_into_pdf"]
PARAM_DEG_GENERAL = config["general_threshold"]
PARAM_DEG_LOW_EXPRESSED = config["low_expression_count"]
PARAM_DEG_NOISEQ_BIO_MODE = config["noiseq_assume_biological_replicates"]
PARAM_DEG_SCORE_THRESHOLD = config["score_threshold"]
PARAM_DEG_STRICT_MODE = config["enable_strict_mode"]
PARAM_DEG_WEIGHT_BAYSEQ = config["weight_bayseq"]
PARAM_DEG_WEIGHT_DESEQ2 = config["weight_deseq2"]
PARAM_DEG_WEIGHT_EDGER = config["weight_edger"]
PARAM_DEG_WEIGHT_LIMMA = config["weight_limma"]
PARAM_DEG_WEIGHT_NOISEQ = config["weight_noiseq"]
PARAM_DEG_WEIGHT_SLEUTH = config["weight_sleuth"]
PARAM_FLEXBAR_LENGTH = config["flexbar_min_length"]
PARAM_FLEXBAR_QUAL = config["flexbar_min_qual"]
PARAM_FLEXBAR_UNCALLED = config["flexbar_max_uncalled"]
PARAM_GENESCF_GO_DB = config["genescf_go_db"]
PARAM_GENESCF_KEGG_DB = config["genescf_kegg_db"]
PARAM_GENESCF_TOTAL_GENES_SPECIES = config["genescf_total_genes_species"]
PATH_FLEXBAR = config["flexbar_path"]
PATH_GENESCF = config["geneSCF_path"]
REF_ANNOTATION = config["ref_annotation_file"]
REF_ANNOTATION_FEATURE_ID = config["ref_annotation_feature_type"]
REF_ANNOTATION_GENE_ID = config["ref_annotation_gene_type"]
REF_FASTA = config["ref_fasta_file"]
REF_GO_FILE = config["ref_go_obo_file"]
REF_INDEX = config["ref_index_name"]
REF_TRANSCRIPTOME = config["ref_fasta_transcriptome"]

# Functions
def create_presence_absence_matrix():
	presence_absence_matrix = open("deg/filtered_gene_counts_presence_absence_matrix.csv", "w") 
	with open("deg/filtered_gene_counts_raw_extended.csv") as raw_counts_table:
		for raw_counts_line in csv.reader(raw_counts_table, delimiter = ","):
			id = raw_counts_line[0]
			gene = raw_counts_line[1]
			product = raw_counts_line[2]
			counts_sample_1 = raw_counts_line[3]
			counts_sample_2 = raw_counts_line[4]
			counts_sample_3 = raw_counts_line[5]
			counts_sample_4 = raw_counts_line[6]
			counts_sample_5 = raw_counts_line[7]
			counts_sample_6 = raw_counts_line[8]
			if id == "ID":
				presence_absence_matrix.write("ID,gene name,product," + counts_sample_1 + "," + counts_sample_2 + "," + counts_sample_3 + "," + counts_sample_4 + "," + counts_sample_5 + "," + counts_sample_6 + "\n")
			else:
				if float(counts_sample_1) > 0:
					counts_sample_1 = "1"
				if float(counts_sample_2) > 0:
					counts_sample_2 = "1"
				if float(counts_sample_3) > 0:
					counts_sample_3 = "1"
				if float(counts_sample_4) > 0:
					counts_sample_4 = "1"
				if float(counts_sample_5) > 0:
					counts_sample_5 = "1"
				if float(counts_sample_6) > 0:
					counts_sample_6 = "1"
				presence_absence_matrix.write(id + "," + gene + "," + product + "," + counts_sample_1 + "," + counts_sample_2 + "," + counts_sample_3 + "," + counts_sample_4 + "," + counts_sample_5 + "," + counts_sample_6 + "\n")
	presence_absence_matrix.close()

def fetch_annotation_counts(count_file, gff_file, annotation_feature, id_tag):
	count_file = "deg/" + count_file
	counts_extended = open(count_file.split(".csv")[0] + "_extended.csv", "w") 
	with open(count_file) as count_values:
		for count_line in csv.reader(count_values, delimiter = ","):
			gene = ""
			product = ""
			id = count_line[0]
			if id == "":
				counts_extended.write("ID,gene name,product," + count_line[1] + "," + count_line[2] + "," + count_line[3] + "," + count_line[4] + "," + count_line[5] + "," + count_line[6] + "\n")
			else:
				with open(gff_file) as gff_annotation:
					for gff_line in csv.reader(gff_annotation, delimiter = "\t"):
						if gff_line[0][0] != "#":
							if len(gff_line) > 8 and gff_line[2] == annotation_feature:
								current_id = gff_line[8].split(id_tag + "=")[1].split(";")[0]
								if id == current_id:
									if "product=" in gff_line[8]:
										product = gff_line[8].split("product=")[1]
									else:
										product = ""
									if gff_line[8].split("ID=")[1].count("gene=") > 0:
										gene = gff_line[8].split("gene=")[1].split(";")[0]
									break
				counts_extended.write(count_line[0] + "," + gene + "," + product + "," + count_line[1] + "," + count_line[2] + "," + count_line[3] + "," + count_line[4] + "," + count_line[5] + "," + count_line[6] + "\n")
	counts_extended.close()

def fetch_annotation_diffexpr(gff_file, annotation_feature, id_tag):
	consensus_extended = open("deg/consensus_diffexpr_results_extended.csv", "w") 
	with open("deg/consensus_diffexpr_results.csv") as consensus_results:
		for consensus_line in csv.reader(consensus_results, delimiter = ","):
			direction = ""
			fold_change = ""
			gene = ""
			product = ""
			id = consensus_line[0]
			if id == "":
				consensus_extended.write("ID,gene name,product," + consensus_line[1] + "," + consensus_line[2] + "," + consensus_line[3] + "," + consensus_line[4] + "," + consensus_line[5] + "," + consensus_line[6] + ",ordering,log2FC\n")
			else:
				with open(gff_file) as gff_annotation:
					for gff_line in csv.reader(gff_annotation, delimiter = "\t"):
						if gff_line[0][0] != "#":
							if len(gff_line) > 8 and gff_line[2] == annotation_feature:
								current_id = gff_line[8].split(id_tag + "=")[1].split(";")[0]
								if id == current_id:
									if "product=" in gff_line[8]:
										product = gff_line[8].split("product=")[1]
									else:
										product = ""
									if gff_line[8].split("ID=")[1].count("gene=") > 0:
										gene = gff_line[8].split("gene=")[1].split(";")[0]
									break
				with open("deg/diffexpr_results_bayseq.csv") as bayseq_results:
					for bayseq_line in csv.reader(bayseq_results, delimiter = ","):
						if id == bayseq_line[0] or id == bayseq_line[1]:
							direction = bayseq_line[9]
							break
				with open("deg/diffexpr_results_limma.csv") as limma_results:
					for limma_line in csv.reader(limma_results, delimiter = ","):
						if id == limma_line[0]:
							fold_change = limma_line[1]
							break
				consensus_extended.write(consensus_line[0] + "," + gene + "," + product + "," + consensus_line[1] + "," + consensus_line[2] + "," + consensus_line[3] + "," + consensus_line[4] + "," + consensus_line[5] + "," + consensus_line[6] + "," + direction + "," + fold_change + "\n")
	consensus_extended.close()

def fetch_gene_symbols(new_file):
	list_of_gene_symbols = open(new_file, "w")
	with open("deg/consensus_diffexpr_results_extended.csv") as consensus_list:
		for line in csv.reader(consensus_list, delimiter = ","):
			if line[1] != "":
				list_of_gene_symbols.write(line[1].split("_")[0] + "\n")
	list_of_gene_symbols.close()

def read_tsv(tsv_filename):
	samples_dic = {}
	with open("raw/" + tsv_filename) as tsv:
		for line in csv.reader(tsv, delimiter = "\t"):
			if line[0][0] != "@":
				samples_dic[line[0]] = line[1]
	return(samples_dic)

# Samples
SAMPLES_AND_CONDITIONS = read_tsv(METADATA)
SAMPLES = SAMPLES_AND_CONDITIONS.keys()

rule DEG_postprocessing:
	input:
		"deg/pathway_analysis_GO",
		"deg/pathway_analysis_KEGG"
	shell:
		"""
		rm deg/consensus_diffexpr_results.csv
		mv deg/consensus_diffexpr_results_extended.csv deg/consensus_diffexpr_results.csv
		rm deg/filtered_gene_counts_raw.csv
		mv deg/filtered_gene_counts_raw_extended.csv deg/filtered_gene_counts_raw.csv
		rm deg/filtered_gene_counts_tpm.csv
		mv deg/filtered_gene_counts_tpm_extended.csv deg/filtered_gene_counts_tpm.csv
		mkdir -p deg/{EXPERIMENT_NAME}
		mv deg/consensus* deg/{EXPERIMENT_NAME}
		mv deg/deg* deg/{EXPERIMENT_NAME}
		mv deg/diffexpr* deg/{EXPERIMENT_NAME}
		mv deg/filtered* deg/{EXPERIMENT_NAME}
		mv deg/pathway* deg/{EXPERIMENT_NAME}
		mv deg/transcript* deg/{EXPERIMENT_NAME}
		"""

# GeneSCF 1.1-p2
# Performs a GO and KEGG overrepresentation analysis on the DEG list
# TO-DO: Test different GO and KEGG dbs
rule pathway_analysis:
	input:
		"deg/deg_gene_symbols"
	output:
		"deg/pathway_analysis_GO",
		"deg/pathway_analysis_KEGG"
	shell:
		"""
		mkdir -p {output[0]}
		mkdir -p {output[1]}		
		{PATH_GENESCF} -m=update -i={input} -t=sym -o={output[0]} -db=GO_all -p=yes -bg={PARAM_GENESCF_TOTAL_GENES_SPECIES} -org={PARAM_GENESCF_GO_DB}
		{PATH_GENESCF} -m=update -i={input} -t=sym -o={output[1]} -db=KEGG -p=yes -bg={PARAM_GENESCF_TOTAL_GENES_SPECIES} -org={PARAM_GENESCF_KEGG_DB}
		# Update GeneSCF results using the GO DB Reference File
		python libraries/genescf_update.py -d {PARAM_GENESCF_GO_DB} -o {REF_GO_FILE}
		"""

rule pathway_analysis_preprocessing:
	input:
		"deg/deg_analysis_graphs.pdf"
	output:
		"deg/deg_gene_symbols"
	run:
		# Fetching initial annotations
		fetch_annotation_counts("filtered_gene_counts_raw.csv", REF_ANNOTATION, REF_ANNOTATION_FEATURE_ID, REF_ANNOTATION_GENE_ID)
		fetch_annotation_counts("filtered_gene_counts_tpm.csv", REF_ANNOTATION, REF_ANNOTATION_FEATURE_ID, REF_ANNOTATION_GENE_ID)
		fetch_annotation_diffexpr(REF_ANNOTATION, REF_ANNOTATION_FEATURE_ID, REF_ANNOTATION_GENE_ID)
		# Create a list of gene symbols for the DEGs
		fetch_gene_symbols(output[0])
		# Generate the presence-absence matrix
		create_presence_absence_matrix()

# baySeq 2.16.0
# DESeq2 1.22.1
# edgeR 3.24.1
# limma 3.38.3
# NOISeq 2.26.0
# sleuth 0.30.0
# UpsetR 1.3.3
rule DEG_analysis:
	input:
		expand("mapped/bowtie2/featureCounts/{sample}/", sample = SAMPLES)
	output:
		"deg/deg_analysis_graphs.pdf"
	conda:
		"libraries/score_deg_environment.yml"
	shell:
		"""
		# Fetch transcript lengths using another python script
		python libraries/fetch_transcript_lengths.py -f {REF_ANNOTATION} -i {REF_ANNOTATION_GENE_ID}
		# Idea: Rscript <folder>/SCORE.R <SAMPLES>
		Rscript libraries/SCORE.R {METADATA} {PARAM_GENESCF_TOTAL_GENES_SPECIES} {MERGE_PLOTS_AND_PDF} {PARAM_DEG_GENERAL} {PARAM_DEG_LOW_EXPRESSED} {PARAM_DEG_WEIGHT_BAYSEQ} {PARAM_DEG_WEIGHT_DESEQ2} {PARAM_DEG_WEIGHT_EDGER} {PARAM_DEG_WEIGHT_LIMMA} {PARAM_DEG_WEIGHT_NOISEQ} {PARAM_DEG_WEIGHT_SLEUTH} {ENABLE_BENCHMARKING} {PARAM_DEG_STRICT_MODE} {PARAM_DEG_NOISEQ_BIO_MODE} {PARAM_DEG_SCORE_THRESHOLD}
		"""
		
# featureCounts 1.6.4
# Counts mapped reads to genomic features
# Needed for the quantification of Bowtie2 results
# Discards multi-mapping reads by default
# TO-DO: Break iff 0 counts in total! (Warning else?)
rule counting:
	input:
		"mapped/bowtie2/{sample}.sam"
	output:
		"mapped/bowtie2/featureCounts/{sample}/"
	conda:
		"libraries/score_count_environment.yml"
	threads:
		4
	shell:
		"""
		featureCounts -T {threads} -a {REF_ANNOTATION} -o counts_{wildcards.sample} {input} -t {REF_ANNOTATION_FEATURE_ID} -g {REF_ANNOTATION_GENE_ID}
		mv counts_{wildcards.sample}* {output}
		"""

# Bowtie2 2.3.4.3
# Kallisto 0.45.0
# Creates a reference index for each genome
# Performs an ungapped genome mapping
# Bowtie2 allows up to 2 mismatches (default)
# Moves the alignment reference index file afterwards, since it's not used anymore
# Followed by quanitification of transcripts (counting of reads)
# TO-DO: Verify that mapping went well?
rule mapping:
	input:
		"trimmed/{sample}_trimmed_1.fastq.gz",
		"trimmed/{sample}_trimmed_2.fastq.gz"
	output:
		"mapped/bowtie2/{sample}.sam"
	conda:
		"libraries/score_map_environment.yml"
	threads:
		4
	shell:
		"""
		bowtie2-build {REF_FASTA} {REF_INDEX}_{wildcards.sample}
		bowtie2 -q --phred33 -p {threads} --no-unal -x {REF_INDEX}_{wildcards.sample} -1 {input[0]} -2 {input[1]} -S {output}
		kallisto index -i {REF_INDEX}_{wildcards.sample}_kallisto.idx {REF_TRANSCRIPTOME}
		kallisto quant -i {REF_INDEX}_{wildcards.sample}_kallisto.idx -o mapped/kallisto/{wildcards.sample}/ -b 100 -t {threads} {input[0]} {input[1]}
		rm {REF_INDEX}_{wildcards.sample}*
		"""

# FastQC 0.11.8
# Flexbar 3.4.0
# Quality control and basequality trimming
# TO-DO: Should not forget adapter trimming when using new data!
# TO-DO: Create Conda environment for Flexbar
rule quality_control_and_trimming:
	input:
		"raw/{sample}_1.fastq.gz",
		"raw/{sample}_2.fastq.gz"
	output:
		"trimmed/{sample}_trimmed_1.fastq.gz",
		"trimmed/{sample}_trimmed_2.fastq.gz"
	conda:
		"libraries/score_qc_environment.yml"
	log:
		"trimmed/logs/"
	threads:
		4
	shell:
		"""
		# Initial FastQC
		mkdir -p fastqc/{wildcards.sample}/
		fastqc {input} -o fastqc/{wildcards.sample}/
		# Trimming
		{PATH_FLEXBAR} -r {input[0]} -p {input[1]} -t {wildcards.sample}_trimmed -n {threads} -u {PARAM_FLEXBAR_UNCALLED} -q TAIL -qf sanger -qt {PARAM_FLEXBAR_QUAL} -m {PARAM_FLEXBAR_LENGTH} -z GZ
		mv {wildcards.sample}_trimmed_1.fastq.gz trimmed/
		mv {wildcards.sample}_trimmed_2.fastq.gz trimmed/
		mv {wildcards.sample}_trimmed.log {log}
		mkdir -p fastqc/{wildcards.sample}_trimmed/
		# Second FastQC
		fastqc trimmed/{wildcards.sample}_trimmed_1.fastq.gz -o fastqc/{wildcards.sample}_trimmed/
		fastqc trimmed/{wildcards.sample}_trimmed_2.fastq.gz -o fastqc/{wildcards.sample}_trimmed/
		"""
		
onstart:
	print("\n Welcome to SCORE: Smart Consensus Of RNA-Seq Expression pipelines")
	print(" Please ensure all input files are located within the raw/ folder and your parameters have been set accordingly.")		
		
onerror:
	print("\n It seems like something went wrong. Please refer to the error messages listed above for additional information. \n")

onsuccess:
	print("\n SCORE finished successfully. \n")